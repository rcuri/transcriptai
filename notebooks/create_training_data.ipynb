{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_required = True\n",
    "video_start_time = \"00:00:32.00\"\n",
    "video_end_time = \"\"\n",
    "video_file_path = \"\"\n",
    "video_output_directory = \"~/training_output/videos/\"\n",
    "transcript_output_directory = \"~/training_output/transcripts/\"\n",
    "audio_output_directory = \"~/training_output/audio/\"\n",
    "speaker_diarization_output_directory = \"~/training_output/speaker_diarization/\"\n",
    "audio_subclip_output_directory = \"~/training_output/subclips/\"\n",
    "json_output_directory = \"~/training_output/full-run-json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transcriptai.video import trim_video_file, convert_mp4_to_mp3\n",
    "import os\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path_posix = Path(video_file_path)\n",
    "original_video_file_name = video_path_posix.stem\n",
    "video_file_name = video_path_posix.name\n",
    "original_video_path = str(video_path_posix.absolute())\n",
    "video_to_process_file_path = os.path.join(video_output_directory, video_file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim video file if required\n",
    "Some videos start with content we don't care about, such as ads before starting the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trim_required:\n",
    "    trim_video_file(\n",
    "        original_video_path, video_to_process_file_path,\n",
    "        video_start_time, video_end_time\n",
    "    )\n",
    "else:\n",
    "    shutil.copyfile(original_video_path, video_to_process_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert video to audio file\n",
    "Convert video to audio file to make file we work with smaller since we only care about the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import AudioFileClip\n",
    "audio_file_clip = AudioFileClip(video_to_process_file_path)\n",
    "\n",
    "audio_file_name = original_video_file_name + \".mp3\"\n",
    "converted_audio_path = audio_output_directory + audio_file_name\n",
    "audio_file_clip.write_audiofile(converted_audio_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply speaker diarization to audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcriptai.audio import apply_speaker_diarization_to_audio_file, transcribe_audio_file\n",
    "\n",
    "hf_access_token = os.getenv(\"HUGGING_FACE_WRITE_ACCESS_TOKEN\")\n",
    "diarization_result_path = apply_speaker_diarization_to_audio_file(\n",
    "    converted_audio_path, \n",
    "    speaker_diarization_output_directory, \n",
    "    hf_access_token\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get annotations from speech diarization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya_speech\n",
    "\n",
    "annotations = malaya_speech.extra.rttm.load(diarization_result_path)\n",
    "sample_name = list(annotations.keys())[0]\n",
    "sample = annotations[sample_name]\n",
    "speaker_map = {\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group segments by chunks less than 90 seconds\n",
    "We want to group segments by chunks less than 90 seconds because we want to split up the audio into matching chunks. OpenAI's 'Whisper' model starts giving weird results when the audio is too long, so I found 90 seconds to be a sweet spot for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_run_time = 0\n",
    "rttm_all_segments = []\n",
    "current_segment_section = {\n",
    "    \"segment_chunks\": []\n",
    "}\n",
    "\n",
    "for segment, track, label in sample.itertracks():\n",
    "    segment_track = {\n",
    "        \"track\": track,\n",
    "        \"segment_start_time\": segment.start,\n",
    "        \"segment_end_time\": segment.end,\n",
    "        \"speaker\": label,\n",
    "    }    \n",
    "    current_seg_run_time = segment.end - segment.start\n",
    "    if current_seg_run_time + current_run_time < 90:\n",
    "        current_segment_section['segment_chunks'].append(segment_track)\n",
    "        current_run_time += current_seg_run_time\n",
    "    else:\n",
    "        rttm_all_segments.append(current_segment_section)\n",
    "        current_segment_section = {\n",
    "            'segment_chunks': []\n",
    "        }\n",
    "        current_segment_section['segment_chunks'].append(segment_track)\n",
    "        current_run_time = current_seg_run_time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add start and end time to full segment chunk\n",
    "Now that we have split up the segments into chunks, we want to get the start time and end time for the whole segment chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "for rttm_chunk in rttm_all_segments:\n",
    "    first_chunk = rttm_chunk['segment_chunks'][0]\n",
    "    # Start time\n",
    "    if first_chunk['track'] == 0:\n",
    "        rttm_chunk['segment_start_time'] = 0\n",
    "        rttm_chunk['clip_start_time'] = 0\n",
    "    else:\n",
    "        rttm_chunk['segment_start_time'] = first_chunk['segment_start_time']\n",
    "        rttm_chunk['clip_start_time'] = math.floor(first_chunk['segment_start_time'])\n",
    "    \n",
    "    # End time\n",
    "    if len(rttm_chunk['segment_chunks']) == 1:\n",
    "        rttm_chunk['segment_end_time'] = first_chunk['segment_end_time']\n",
    "        rttm_chunk['clip_end_time'] = math.ceil(first_chunk['segment_end_time'])       \n",
    "    else:\n",
    "        last_chunk = rttm_chunk['segment_chunks'][-1]\n",
    "        rttm_chunk['segment_end_time'] = last_chunk['segment_end_time']\n",
    "        rttm_chunk['clip_end_time'] = math.ceil(last_chunk['segment_end_time'])       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create directory to store all subclips for current video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subclip_directory = os.path.join(audio_subclip_output_directory, original_video_file_name)\n",
    "os.makedirs(new_subclip_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create subclips for each full segment chunk\n",
    "Now that we know the length for each segment, we want to create an audio subclip for each segment so that we can analyze it with OpenAI's transcribe endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rttm_chunk in rttm_all_segments:\n",
    "    subclip_start_time = rttm_chunk['clip_start_time']\n",
    "    subclip_end_time = rttm_chunk['clip_end_time']\n",
    "    subclip_file_name = str(subclip_start_time) + \"-\" + str(subclip_end_time) + audio_file_name\n",
    "    subclip_file_path = os.path.join(new_subclip_directory, subclip_file_name)\n",
    "    audio_subclip = audio_file_clip.subclip(subclip_start_time, subclip_end_time)\n",
    "    audio_subclip.write_audiofile(subclip_file_path)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe all audio files in new subclip directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_transcript_directory = os.path.join(transcript_output_directory, original_video_file_name)\n",
    "os.makedirs(current_transcript_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transcriptai.audio import transcribe_audio_file\n",
    "\n",
    "all_subclips = os.listdir(new_subclip_directory)\n",
    "all_subclips.sort()\n",
    "\n",
    "for subclip_file in all_subclips:\n",
    "    current_subclip_file_path = os.path.join(new_subclip_directory, subclip_file)\n",
    "    transcript = transcribe_audio_file(current_subclip_file_path)\n",
    "    raw_transcript_text = transcript['text']\n",
    "\n",
    "    transcript_text_file_name = subclip_file + \".txt\"\n",
    "    transcript_file_path = os.path.join(\n",
    "        current_transcript_directory, transcript_text_file_name\n",
    "    )\n",
    "    with open(transcript_file_path, \"w\") as f:\n",
    "        f.write(raw_transcript_text)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save run JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_output = {\n",
    "    \"output\": rttm_all_segments\n",
    "}\n",
    "json_file_name = original_video_file_name + \".json\"\n",
    "json_file_path = os.path.join(json_output_directory, json_file_name)\n",
    "with open(json_file_path, \"w\") as outfile:\n",
    "    json.dump(json_output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbatranscripts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 (default, May 15 2023, 19:36:23) \n[Clang 13.0.0 (clang-1300.0.27.3)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "948f5a465bd6da6fc43dc168a7d6d823f9345b0b70e639577a2ec592454248cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
